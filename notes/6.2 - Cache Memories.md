# Caching in the Memory Hierarchy

![[截屏2022-10-24 上午10.41.06.png | 400]]

## Block Sizes
- Block sizes are usually the same for adjoining levels
- However, this is not the case for non-adjoining levels
	- Reg - L1 Cache: 1 word
	- L1 Cache - L2 Cache: a few dozen bytes
	- RAM - Disk: a few hundred/thousand bytes

## Cache Hit & Cache Miss
### Cache Hit

### Cache Miss
Replacing, Victim Block & Replacement Strategy

1. Cold Miss
2. Conflict Miss
3. Capacity Miss


# Internal Structure of Cache Memories

![[截屏2022-10-24 上午10.50.01.png | 400]]
- Sets
	- Cache Lines
		- Valid Bit
			- `1` for valid, `0` for non-valid
		- Tag
			- Is target data in the current cache line?
		- Cache Block

### Describing Cache
- `(S, E, B, m)`
- **The capacity of cache** is that of all of its cache blocks,  `C = S⋅E⋅B`


# How Cache Works
1. CPU sends memory address A to L1 cache
2. If L1 cache holds the demanded data, that data is directly sent to the CPU

## How does cache knows what data it holds?

![[截屏2022-10-29 下午1.53.10.png | 300]]

The memory address is split into 3 parts:
1. Tag - t bits
2. Set Index - s bits
3. Block Index - b bits

Locating data:
1. **Set index** determines which set the data is in
2. The **tag** and **valid bit** work together to indicate which cache line the data is in
3. The **block index** indicate where in a single cache block is the data

### Why use the middle bits for set index?
- If the top bits are used, consecutive data in the memory are mapped to the same set
- When accessing an array, line switching happens at each element access, causing poor performance; also only a few set is utilized


# Types of Cache
Categorized by the number of lines per set

## Direct-Mapped Caches
1 Line per Set

### Determining a Match
1. Set Selection
	- The set index bits are parsed as unsigned numbers
2. Line Matching
	- If `(validBit == 1 && addressTag == setTag)`, then target data is in the current line
	- Else, a cache miss occurs, and the CPU demand data from one level down
	- Replacement strategy: use the newly extracted line to replace the old line
3. Word Extraction
	- ![[截屏2022-10-24 上午11.06.01.png | 500]]

### DMC in Action
- Different regions of memory can be mapped to the same cache set with the **tag**
	- Might cause **conflict misses**
	- A situation where a single cache block is alternating between two regions in memory, always missing is called **thrashing**
	- Preventing thrashing by inserting padding between arrays
- Each cache line may hold more data than is requested for a single memory access due to **block offset**


## Set Associative Caches
`1 < E < C/B`

### Determining a Match
When performing line matching, traverse all tags in hope of finding a match
![[截屏2022-10-24 上午11.24.12.png | 500]]

### Replacement Strategy
1. Empty lines, i.e. line where `validBit == 0`
2. If no empty lines exist:
	1. Random Replacement
	2. Least Frequently Used
	3. Least Recently Used

	- The latter two strategies have additional overheads, however, as the penalty for cache miss gets more severe the lower we are on the memory hierarchy, this overhead is preferable


## Fully Associative Caches
Only one set, `E = C/B`

### Determining a Match
Set index is no longer needed
![[截屏2022-10-24 上午11.31.37.png | 500]]

Due to cost constraints, FAC is only good for small-scale cache memories


# Writing to Cache
## Write Hit
### `write-through`
- CPU writes to the cache as well as to one lever lower
- Makes it ok for the cache to just discard victim data

### `write-back`
- Only write to cache
- Write to one level lower when eviction occurs
- Each cache line needs to have an additional **modification bit**

## Write Miss
### `write-allocate`
- Load data to cache, than write to cache

### `no-write-allocate`
- Writes data directly to one level lower

Usually `write-through` goes with `no-write-allocate`, and `write-back` goes with `write-allocate`


# Cache Systems IRL

Intel Core i7
![[截屏2022-10-24 下午12.11.32.png | 500]]
![[截屏2022-10-24 下午12.13.37.png | 500]]

## i-cache & d-cache
i-cache: only stores instructions
d-cache: only stores data
unified cache: stores both instructions and data

Using both d-cache and i-cache allows the CPU to read both instructions and data at the same time, while preventing conflicts and misses


# Cache Performance
## Metrics
1. Miss Rate
2. Hit Rate
3. Hit Time
4. Miss Penalty

## Influences
1. **Larger capacity** cache **increases hit rate** while decreasing speed, leading to **longer hit times**
2. **Larger blocks** make use of **spacial locality** to increase hit rates. However, this means fewer lines per set, **harming temporally local** programs' hit rate. Besides, larger blocks have **harsher miss penalties** for misses.
3. **More lines/Higher associativity** decreases the possibility for conflict misses, but capping speed.
4. **Write strategies**: usually L1 - L2 use write-through while lower levels tend to use write-back


# Writing Cache-Friendly Code
## Principals
1. Focus on the code that gets run the most, e.g. inner loop.
2. Make the miss rate as low as possible for these code segments.

## Miss Rate in Loops
For a cache with a block size of $B$ bytes, a reference pattern with a stride of $k$ words, and a word size of $w$, the average miss rate is
$$min\{1, wk/B\} \text{ misses per loop iteration}$$

**e.g.**
v is block aligned, words are 4 bytes, cache blocks are 4 words, and the cache is initially empty

![[截屏2022-10-24 下午1.00.02.png]]

### Summary
- Repeated references to local variables are good
- Stride-1 reference patterns are good

## Multi-Dimensional Arrays

![[截屏2022-10-24 下午1.03.40.png | 400]]

**Row-major access:**
![[截屏2022-10-24 下午1.05.24.png]]
**Column-major access if array is larger than cache:**
![[截屏2022-10-24 下午1.06.03.png]]


# How Cache Affects Performance
## The Memory Mountain
**Read Throughput**: can be used to access memory system performance

![[截屏2022-10-24 下午1.27.37.png]]
- Ridges of temporal locality: where the working set can fit within a memory level
 ![[截屏2022-10-24 下午1.30.46.png | 400]]
- Slope of spatial locality: for s8 and above, access request is destined to miss in L2 and must reach L3, and the speed is restrained by the speed of L3 - L2 transfer
![[截屏2022-10-24 下午1.34.47.png | 400]]
- Prefetching mechanism: works for small strides, accounts for flat lines

## Rearranging Loops to Increase Spatial Locality
**e.g.**  Matrix multiplication
![[截屏2022-10-24 下午2.51.03.png | 500]]
1. Iterating i is stride-n
2. j: stride-1
3. k: stride-1 and stride-n

![[截屏2022-10-24 下午2.51.48.png | 500]]
![[截屏2022-10-24 下午2.55.09.png | 500]]
**Keep the loop with the smallest stride as the innermost loop**

## Blocking to Increase Temporal Capacity
Processes blocks of data, that can fit into a certain memory level, at a time and discard them after the processing is done

![[截屏2022-10-24 下午2.57.35.png]]
![[截屏2022-10-24 下午2.57.50.png]]